# CUDA_VISIBLE_DEVICES for Nvidia GPU, refers to GPU id, starts at 0
CUDA_VISIBLE_DEVICES=0

# Flag to enable local debugging
DEBUGGING=True

# DEPLOYMENT_ENV=localhost or aws
DEPLOYMENT_ENV=localhost

# DEPLOYMENT_MODULE
DEPLOYMENT_MODULE=tensorflow-serving

# Docker configurations
DOCKER_USER=luxurymaster
DOCKER_REPO=luckma

# Tensorflow memory configuration
TF_GPU_MEMORY_LIMIT=4096

# Train configurations
TRAIN_PREDICATE=05-21-20

# Local configurations
MODEL_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\model
MODEL_EXPORT_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\model_export

INPUT_CONFIG_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\input\config
INPUT_DATA_PATH=D:\luckma\data\processed
INPUT_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\input

OUTPUT_DATA_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\output\data
OUTPUT_PATH=C:\Users\luxurymaster\PycharmProjects\luckma-ats-docker\ml\output


# AWS configurations
# MODEL_PATH=/opt/ml/model
# INPUT_PATH=/opt/ml/input
# INPUT_DATA_PATH=/opt/ml/input/data
# OUTPUT_PATH=/opt/ml/output
# INPUT_CONFIG_PATH=/opt/ml/input/config
# OUTPUT_DATA_PATH=/opt/ml/output/data
